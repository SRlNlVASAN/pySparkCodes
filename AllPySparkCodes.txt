[py only]

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('demo').getOrCreate()
a = [(1,'sam'),(2,'eam'),(3,'ram'),(4,'jam')]
b = ['id','name']

df = spark.createDataFrame(a,b)

df.display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[sql only]

%sql
create or replace temporary view demo as
select 1 as id, 'sam' as name union
select 2 as id, 'ram' as name union
select 3 as id, 'fam' as name union
select 4 as id, 'jam' as name;

select * from demo;
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[python and sql]

from pyspark.sql.functions import col
a = [(1,'sam'),(2,'ram'),(3,'fam')]
b = ['id','name']

df = spark.createDataFrame(a,b)

df.createOrReplaceTempView('demo')

%sql

select * from demo;
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[read a csv]

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('demo').getOrCreate()

df = spark.read.option('header',True).option('inferSchema',True).csv('/Volumes/workspace/learningfiles/veh_data/car_prices.csv')

df.display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[use withColumn for alter column and concat]

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('demo').getOrCreate()

df = spark.read.option('header','true').option('inferSchema','true').csv('/Volumes/workspace/learningfiles/veh_data/borewellVehData.csv')

df.display()

# for create new col
from pyspark.sql.functions import col,lit,concat_ws
df2 = df.withColumn('addLatLon',col('lat') + col('lon'))

df2.display()

# modify in existing col

df3 = df2.withColumn('addLatLon',col('addLatLon') - 50)

df3.display()

#concat two col in pyspark

df3 = df3.withColumn('newlatlon',concat_ws('',col('lat'),col('lon')))


df3.display()

#to read col based on list

df = spark.read.option('header',True).option('inferSchema',True).csv('/Volumes/workspace/learningfiles/veh_data/borewellVehData.csv')

colm = [col for col in df.columns]

df.select(*colm).display()

#to read col in index values

df = spark.read.option('header',True).option('inferSchema',True).csv('/Volumes/workspace/learningfiles/veh_data/borewellVehData.csv')

#df.select(df.columns[0:3]).display()

# for selecting all columns
df.select(df.columns).display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[to create datatype manually]

from pyspark.sql.types import StructType,StructField, StringType, IntegerType
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("demo").getOrCreate()

a = [
    (1,'ram',20),
     (2,'jam',30),
     (3,'kam',20)
     ]

b = StructType([
    StructField('id',IntegerType(),True),
    StructField('name',StringType(),True),
    StructField('age',IntegerType(),True),
])

df = spark.createDataFrame(a,b)

df.display()

[NESTED datatype creating]

from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("demo").getOrCreate()

a = [(1,[{'firstname':'babu','lastname':'sam'}])]

b = StructType([
    StructField('id',IntegerType(),True),
    StructField('name',ArrayType(StructType([
        StructField('firstname',StringType(),True),
        StructField('lastname',StringType(),True)
    ])),True)
])  

df = spark.createDataFrame(a,b)

df.display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[for using filter in pyspark]

from pyspark.sql import SparkSession
from pyspark.sql.types import *

spark = SparkSession.builder.appName("demo").getOrCreate()

a  = [
    ('ORD1001','karan',['mouse','keyboard','monitor']),
    ('ORD1002','meena',['laptop','bag']),
    ('ORD1003','john',['charger','usb cable']),
    ('ORD1004',None,[]),
    ('ORD1005','raj',['mouse','laptop','pen drive'])
]

b = StructType([
    StructField('OrderID',StringType(),True)
    ,StructField('CustomerName',StringType(),True)
    ,StructField('Products',ArrayType(StringType()),True)

])

df = spark.createDataFrame(a,b)

#single filter
#df.filter((df.OrderID == 'ORD1001').display()
          
#multi filter
df.filter((df.OrderID == 'ORD1002') | (df.CustomerName == 'karan')).show()

#is null and is not null
df.filter(df.CustomerName.isNull()).display()

#startswith, endswith and contains

df.filter(df.CustomerName.startswith('m')).display()
print('-------')
df.filter(df.CustomerName.endswith('n')).display()
print('-------')
df.filter(df.CustomerName.contains('aj')).display()

# to get a particular list inside a filter
order_failure = ['ORD1004','ORD1005']
df.filter(df.OrderID.isin(order_failure)).show()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[for withcolumn rename()]

# withcolumnrename

df = spark.read.option('header','true').option('inferschema','true').csv('/Volumes/workspace/learningfiles/veh_data/borewellVehData.csv')

#column rename (here multiple)
df.withColumnRenamed('id','Table_id')\
    .withColumnRenamed('device_id','did')\
    .limit(5)\
    .display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[distinct nd drop druplicates in pyspark]

from pyspark.sql import SparkSession

a = [
    (1,'alice',25,'chennai'),
    (2,'bob',30,'mumbai'),
    (3,'alice',25,'chennai'),
    (4,'david',40,'delhi'),
    (5,'eve',None,'chennai'),
    (6,'frank',30,None),
    (6,'frank',30,None)
]

b = StructType([
    StructField('id',IntegerType(),True),
    StructField('name',StringType(),True),
    StructField('age',IntegerType(),True),
    StructField('city',StringType(),True)
])


df = spark.createDataFrame(a,b)

df.display() # have all data

df.distinct().display() # have all data

df2 = df.dropDuplicates().display() # have all data
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[sort and orderby]

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from pyspark.sql import functions
from pyspark.sql.functions import col

a = [
    (1,'alice',25,'chennai'),
    (2,'bob',30,'mumbai'),
    (3,'alice',25,'chennai'),
    (4,'david',40,'delhi'),
    (5,'eve',None,'chennai'),
    (6,'frank',30,None),
    (6,'frank',30,None)
]

b = StructType([
    StructField('id',IntegerType(),True),
    StructField('name',StringType(),True),
    StructField('age',IntegerType(),True),
    StructField('city',StringType(),True)
])


df = spark.createDataFrame(a,b)

df.display() #nrml

df.orderBy("name").display() #orderby for shorting

df.orderBy(col("name").desc()).display() #same ordering but in desc order

df.sort('city').display() #sort for shorting
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[groupby in pyspark]

from pyspark.sql.functions import *

df = spark.read.option("header", "true").option("inferSchema", "true").csv('/Volumes/workspace/learningfiles/veh_data/car_prices.csv')

df.display()

#sum
df.groupBy('year').sum('sellingprice').display()
#min/max
df.groupBy('year').max('sellingprice').display()
#avg
df.groupBy('year','make').avg('sellingprice').display()
#mean
df.groupBy('year').mean('sellingprice').display() #mean avg same
#count
df.groupBy('year').count().display()

#to perform all aggrecate function in one block

df1 = df.groupby('year').agg(count('sellingprice'),avg('sellingprice'),max('sellingprice'),min('sellingprice'),sum('sellingprice'))

df1.orderBy(desc('year')).display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[union and union all]

from pyspark.sql.functions import *

a = [(1,'sam',20),(2,'bob',25),(3,'amy',30)]

b = ['id','name','age']

df = spark.createDataFrame(a,b)

c = [(4,'fam',20),(5,'buub',25),(3,'amy',30)]

d = ['id','name','age']

df = spark.createDataFrame(a,b)
df1 = spark.createDataFrame(c,d)

df.union(df1).display() #union unionall same
df.union(df1).dropDuplicates().display() #if want to remove dublicates
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[unionbyname]

from pyspark.sql.functions import *

a = [(1,'sam',20),(2,'bob',25),(3,'amy',30)]
b = ['id','name','age']
df = spark.createDataFrame(a,b)

c = [('fam',4,20,'che'),('sam',5,20,'che'),('bob',6,25,'mdu')]
d = ['name','id','age','city']

df = spark.createDataFrame(a,b)
df1 = spark.createDataFrame(c,d)

display(df)
display(df1)

df.unionByName(df1, allowMissingColumns=True).display() #using this we can union 2 tables even with difference structures
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[dropna and fillna]

from pyspark.sql.functions import *

a = [(1,'alice',None),
     (2,None,2000),
     (3,'charlie',3000),
     (4,None,None)]
     
b = ['id','name','salary']

df = spark.createDataFrame(a,b)

#fillna
#df2 = df.fillna(value= 10,subset=['salary'])

#dropna
df.dropna().display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[when() and otherwise()]


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
