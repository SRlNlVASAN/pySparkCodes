[py only]

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('demo').getOrCreate()
a = [(1,'sam'),(2,'eam'),(3,'ram'),(4,'jam')]
b = ['id','name']

df = spark.createDataFrame(a,b)

df.display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[sql only]

%sql
create or replace temporary view demo as
select 1 as id, 'sam' as name union
select 2 as id, 'ram' as name union
select 3 as id, 'fam' as name union
select 4 as id, 'jam' as name;

select * from demo;
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[python and sql]

from pyspark.sql.functions import col
a = [(1,'sam'),(2,'ram'),(3,'fam')]
b = ['id','name']

df = spark.createDataFrame(a,b)

df.createOrReplaceTempView('demo')

%sql

select * from demo;
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[read a csv]

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('demo').getOrCreate()

df = spark.read.option('header',True).option('inferSchema',True).csv('/Volumes/workspace/learningfiles/veh_data/car_prices.csv')

df.display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[use withColumn for alter column and concat]

from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('demo').getOrCreate()

df = spark.read.option('header','true').option('inferSchema','true').csv('/Volumes/workspace/learningfiles/veh_data/borewellVehData.csv')

df.display()

# for create new col
from pyspark.sql.functions import col,lit,concat_ws
df2 = df.withColumn('addLatLon',col('lat') + col('lon'))

df2.display()

# modify in existing col

df3 = df2.withColumn('addLatLon',col('addLatLon') - 50)

df3.display()

#concat two col in pyspark

df3 = df3.withColumn('newlatlon',concat_ws('',col('lat'),col('lon')))


df3.display()

#to read col based on list

df = spark.read.option('header',True).option('inferSchema',True).csv('/Volumes/workspace/learningfiles/veh_data/borewellVehData.csv')

colm = [col for col in df.columns]

df.select(*colm).display()

#to read col in index values

df = spark.read.option('header',True).option('inferSchema',True).csv('/Volumes/workspace/learningfiles/veh_data/borewellVehData.csv')

#df.select(df.columns[0:3]).display()

# for selecting all columns
df.select(df.columns).display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[to create datatype manually]

from pyspark.sql.types import StructType,StructField, StringType, IntegerType
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("demo").getOrCreate()

a = [
    (1,'ram',20),
     (2,'jam',30),
     (3,'kam',20)
     ]

b = StructType([
    StructField('id',IntegerType(),True),
    StructField('name',StringType(),True),
    StructField('age',IntegerType(),True),
])

df = spark.createDataFrame(a,b)

df.display()

[NESTED datatype creating]

from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("demo").getOrCreate()

a = [(1,[{'firstname':'babu','lastname':'sam'}])]

b = StructType([
    StructField('id',IntegerType(),True),
    StructField('name',ArrayType(StructType([
        StructField('firstname',StringType(),True),
        StructField('lastname',StringType(),True)
    ])),True)
])  

df = spark.createDataFrame(a,b)

df.display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[for using filter in pyspark]

from pyspark.sql import SparkSession
from pyspark.sql.types import *

spark = SparkSession.builder.appName("demo").getOrCreate()

a  = [
    ('ORD1001','karan',['mouse','keyboard','monitor']),
    ('ORD1002','meena',['laptop','bag']),
    ('ORD1003','john',['charger','usb cable']),
    ('ORD1004',None,[]),
    ('ORD1005','raj',['mouse','laptop','pen drive'])
]

b = StructType([
    StructField('OrderID',StringType(),True)
    ,StructField('CustomerName',StringType(),True)
    ,StructField('Products',ArrayType(StringType()),True)

])

df = spark.createDataFrame(a,b)

#single filter
#df.filter((df.OrderID == 'ORD1001').display()
          
#multi filter
df.filter((df.OrderID == 'ORD1002') | (df.CustomerName == 'karan')).show()

#is null and is not null
df.filter(df.CustomerName.isNull()).display()

#startswith, endswith and contains

df.filter(df.CustomerName.startswith('m')).display()
print('-------')
df.filter(df.CustomerName.endswith('n')).display()
print('-------')
df.filter(df.CustomerName.contains('aj')).display()

# to get a particular list inside a filter
order_failure = ['ORD1004','ORD1005']
df.filter(df.OrderID.isin(order_failure)).show()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[for withcolumn rename()]

# withcolumnrename

df = spark.read.option('header','true').option('inferschema','true').csv('/Volumes/workspace/learningfiles/veh_data/borewellVehData.csv')

#column rename (here multiple)
df.withColumnRenamed('id','Table_id')\
    .withColumnRenamed('device_id','did')\
    .limit(5)\
    .display()
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[distinct nd drop druplicates in pyspark]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[]
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
